# BERT

## Word Embedding

One-Hot词向量
Distribution Embedding
Word Piece方法构建词向量

## Self Attention 机制

* query矩阵 Q  
* key矩阵 K  
* value矩阵 V  
* softmax[(Q * K)/sqrt(d)]  
* softmax * v

## Multi-headed机制

## 堆叠多层

## 位置信息编码

## 归一化和残差连接

## Decoder

Mask机制

## 最终输出

## 如何训练BERT

1. 15%随机Mask  
2. 预测两个句子是否应该连在一起  
3. x

## Pre-Training

## Fine-Tunning
